{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "class ChargingEnv():\n",
    "    \"\"\"\n",
    "    ### 动作空间\n",
    "\n",
    "    动作是形状为`(1,)`的`ndarray`, 表示该小时内汽车充放电的功率, 正数代表充电, 复数代表放电.\n",
    "\n",
    "    | 序号 | 动作     | 最小值 | 最大值 |\n",
    "    |-----|----------|--------|--------|\n",
    "    | 0   | power    | -20.0   | 20.0    |\n",
    "\n",
    "\n",
    "    ### 观察空间\n",
    "\n",
    "    观察是形状为`(25,)`的`ndarray`, 表示汽车的当前电量和前24小时电价。\n",
    "\n",
    "    | 序号 | 观察                | 最小值 | 最大值 |\n",
    "    |-----|--------------------|--------|--------|\n",
    "    | 0   | SOC                | 0      | 77     |\n",
    "    | 1   | e_price            | 0      | 200    |\n",
    "\n",
    "    ### 奖励\n",
    "\n",
    "    奖励函数定义为：\n",
    "\n",
    "    *r = -(power * e_price)*\n",
    "\n",
    "    ### 起始状态\n",
    "\n",
    "    起始状态是SOC为77(即满电状态), 电价取决于开始时的电价数据。\n",
    "    \"\"\"\n",
    "    def __init__(self, penalty_factor=0.1):\n",
    "        #环境参数\n",
    "        # 生成上班时间，范围在七点到九点\n",
    "        self.start_time = self.generate_random_time(7, 9)\n",
    "        # 生成下班时间，范围在四点到六点\n",
    "        self.end_time = self.generate_random_time(16, 18)\n",
    "        #计算出实际情况下的开始充电和停止充电时间(整点)\n",
    "        self.real_start_time, self.real_end_time = self.calculate_real_time(self.start_time, self.end_time)\n",
    "        self.battery_capacity = 77\n",
    "        self.SOC = 77\n",
    "        self.soc_min = 0.0\n",
    "        self.soc_max = 77.0\n",
    "        self.e_price_min = 0.0\n",
    "        self.e_price_max = 200.0\n",
    "        self.power_max = 20.0\n",
    "        self.current_step = 0\n",
    "        self.df_prices = pd.read_csv(\"GR-data-11-20.csv\", header=None, names=[\"DateTime\", \"ElectricityPrice\"])\n",
    "        self.penalty_factor = penalty_factor\n",
    "\n",
    "        print(\"length of prices\", len(self.df_prices))\n",
    "\n",
    "        # 观察空间和动作空间的定义\n",
    "        # 定义观察空间\n",
    "        self.observation_space = spaces.Box(low=np.array([self.soc_min, self.e_price_min]),\n",
    "                                            high=np.array([self.soc_max, self.e_price_max]),\n",
    "                                            dtype=np.float32)       \n",
    "\n",
    "        # 定义动作空间\n",
    "        self.action_space = spaces.Box(low=np.array([-self.power_max]),\n",
    "                                       high=np.array([self.power_max]),\n",
    "                                       dtype=np.float32)\n",
    "        \n",
    "    def generate_random_time(self, start_hour, end_hour):\n",
    "        hour = random.randint(start_hour, end_hour)\n",
    "        minute = random.randint(0, 59)\n",
    "        second = random.randint(0, 59)\n",
    "        return datetime.now().replace(hour=hour, minute=minute, second=second)\n",
    "    \n",
    "    def calculate_real_time(self, start_time, end_time):\n",
    "        real_start_time = (start_time + timedelta(hours = 1))\n",
    "        real_start_time = real_start_time.replace(minute=0, second=0)\n",
    "        real_end_time = (end_time - timedelta(hours = 0))\n",
    "        real_end_time = real_end_time.replace(minute=0, second=0)\n",
    "        return real_start_time, real_end_time\n",
    "        \n",
    "    def read_e_price(self,index):  \n",
    "        # Ensure the index is within the range of the dataframe\n",
    "        if 0 <= index < len(self.df_prices):\n",
    "            one_price = self.df_prices[\"ElectricityPrice\"].iloc[index] / 1000\n",
    "            return one_price\n",
    "        else:\n",
    "            # Handle the case where the index is out of range\n",
    "            print(\"Index out of range.\")\n",
    "            return None\n",
    "        \n",
    "    def step(self, power):\n",
    "        \"\"\"\n",
    "        在环境中执行一步动作，并返回新的观察、奖励等信息。\n",
    "\n",
    "        参数：\n",
    "        - `power`：该小时内汽车充电的功率。\n",
    "\n",
    "        返回：\n",
    "        - `observation`：新的观察。\n",
    "        - `reward`：当前步的奖励。\n",
    "        - `done`：标志是否完成（截断剧集）。\n",
    "        - `info`：其他信息（空字典）。\n",
    "        \"\"\"\n",
    "        SOC, e_price = self.state  # th := theta\n",
    "\n",
    "        # 对功率进行裁剪，确保在合理范围内\n",
    "        power = np.clip(power, -self.power_max, self.power_max)[0]\n",
    "        \n",
    "        # 计算新的SOC\n",
    "        newSOC = SOC + power\n",
    "        newSOC = np.clip(newSOC, self.soc_min, self.soc_max)\n",
    "        self.SOC = newSOC\n",
    "\n",
    "        # 计算成本，根据功率和电价\n",
    "        costs = (newSOC - SOC) * e_price\n",
    "        penalty1 = self.penalty_factor * min(0, power) * e_price\n",
    "        penalty2 = self.penalty_factor * max(0, power) * e_price\n",
    "        costs += penalty1\n",
    "        costs -= penalty2\n",
    "\n",
    "        if (self.current_step + 1) % 24 == self.real_start_time.hour or (self.current_step + 1) % 24 == self.real_end_time.hour:\n",
    "            if (self.SOC < 10):\n",
    "                costs += 500\n",
    "                print(\"SOC is less than 10%: \", self.SOC)\n",
    "            \n",
    "        #取出新的电价\n",
    "        newe_price = self.read_e_price(self.current_step+1)\n",
    "        newe_price = np.clip(newe_price, self.e_price_min, self.e_price_max)\n",
    "        self.current_step += 1\n",
    "\n",
    "        self.state = np.array([newSOC, newe_price])\n",
    "\n",
    "        # 返回新的观察、奖励、是否完成、其他信息\n",
    "        return self._get_obs(), -costs, False, False, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境到初始状态。\n",
    "\n",
    "        返回：\n",
    "        - `observation`：初始观察。\n",
    "        - `info`：空字典。\n",
    "        \"\"\"\n",
    "        # 恢复起始状态\n",
    "        # self.current_step = 0\n",
    "        self.state = np.array([77, self.read_e_price(self.current_step)])\n",
    "        \n",
    "        # 返回初始观察和空字典\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        返回当前观察。\n",
    "\n",
    "        返回：\n",
    "        - `observation`：当前观察。\n",
    "        \"\"\"\n",
    "        SOC, e_price = self.state\n",
    "        return np.array([SOC,e_price], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TH\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Agent\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device type: \", device)\n",
    "\n",
    "# Hyperparameters\n",
    "LR_ACTOR = 1e-4\n",
    "LR_CRITIC = 1e-3\n",
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "TAU = 5e-3\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x)) * 20\n",
    "        return x\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add_memo(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "        self.replay_buffer = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = self.actor(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "        # Update critic\n",
    "        next_actions = self.actor_target(next_states)\n",
    "        target_Q = self.critic_target(next_states,\n",
    "                                      next_actions.detach())  # .detach() means the gradient won't be backpropagated to the actor\n",
    "        target_Q = rewards + (GAMMA * target_Q * (1 - dones))\n",
    "        current_Q = self.critic(states, actions)\n",
    "        critic_loss = nn.MSELoss()(current_Q, target_Q.detach())  # nn.MSELoss() means Mean Squared Error\n",
    "        self.critic_optimizer.zero_grad()  # .zero_grad() clears old gradients from the last step\n",
    "        critic_loss.backward()  # .backward() computes the derivative of the loss\n",
    "        self.critic_optimizer.step()  # .step() is to update the parameters\n",
    "\n",
    "        # Update actor \n",
    "        actor_loss = -self.critic(states, self.actor(states)).mean()  # .mean() is to calculate the mean of the tensor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target networks of critic and actor\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
    "            \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of prices 44160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TH\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([64, 64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC is less than 10%:  7.2825445074026405\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  3.4718708847473607\n",
      "Episode: 1, Reward: -1490.29, Best Reward: -1490.29\n",
      "SOC is less than 10%:  0.0\n",
      "Episode: 2, Reward: -487.12, Best Reward: -487.12\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  4.22182444577793\n",
      "SOC is less than 10%:  3.429728775344193\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "Episode: 3, Reward: -2491.5, Best Reward: -487.12\n",
      "SOC is less than 10%:  0.7050676720910545\n",
      "SOC is less than 10%:  0.9927043755394749\n",
      "Episode: 4, Reward: -989.56, Best Reward: -487.12\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  9.630803517063939\n",
      "SOC is less than 10%:  5.697030629558331\n",
      "Episode: 5, Reward: -1480.91, Best Reward: -487.12\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  9.54271517126864\n",
      "SOC is less than 10%:  4.999229155697741\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  7.985685174852652\n",
      "Episode: 6, Reward: -2487.45, Best Reward: -487.12\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.34977097555010417\n",
      "Episode: 7, Reward: -1985.1, Best Reward: -487.12\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  6.026677791742538\n",
      "SOC is less than 10%:  6.901553273459221\n",
      "SOC is less than 10%:  3.2217036314719003\n",
      "SOC is less than 10%:  9.41440277113115\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  8.366664999056162\n",
      "Episode: 8, Reward: -3490.31, Best Reward: -487.12\n",
      "SOC is less than 10%:  1.933662779059759\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "Episode: 9, Reward: -1494.47, Best Reward: -487.12\n",
      "SOC is less than 10%:  1.262180254884914\n",
      "SOC is less than 10%:  5.284633056159045\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "Episode: 10, Reward: -1991.78, Best Reward: -487.12\n",
      "SOC is less than 10%:  9.321745523414503\n",
      "SOC is less than 10%:  0.5123406052589417\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.5694769024848938\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  5.219274689420317\n",
      "Episode: 11, Reward: -2984.58, Best Reward: -487.12\n",
      "SOC is less than 10%:  0.09302672232810494\n",
      "SOC is less than 10%:  1.329878568649292\n",
      "SOC is less than 10%:  6.150822236163956\n",
      "SOC is less than 10%:  1.6919089555740356\n",
      "Episode: 12, Reward: -1991.21, Best Reward: -487.12\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  2.0339990046289884\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "Episode: 13, Reward: -2493.51, Best Reward: -487.12\n",
      "SOC is less than 10%:  2.4606528282165527\n",
      "SOC is less than 10%:  6.888041639964136\n",
      "SOC is less than 10%:  5.953868924641824\n",
      "Episode: 14, Reward: -1489.16, Best Reward: -487.12\n",
      "SOC is less than 10%:  7.017904627936785\n",
      "SOC is less than 10%:  7.573105773575726\n",
      "SOC is less than 10%:  3.264026165008545\n",
      "SOC is less than 10%:  0.3877145792552579\n",
      "SOC is less than 10%:  6.754549570074584\n",
      "Episode: 15, Reward: -2492.23, Best Reward: -487.12\n",
      "SOC is less than 10%:  4.365748405456543\n",
      "SOC is less than 10%:  5.270814160862628\n",
      "SOC is less than 10%:  4.008847236633301\n",
      "Episode: 16, Reward: -1488.45, Best Reward: -487.12\n",
      "SOC is less than 10%:  7.717399597167969\n",
      "Episode: 17, Reward: -490.66, Best Reward: -487.12\n",
      "SOC is less than 10%:  8.906333134978816\n",
      "SOC is less than 10%:  9.455865568879062\n",
      "Episode: 18, Reward: -991.1, Best Reward: -487.12\n",
      "Episode: 19, Reward: 10.38, Best Reward: 10.38\n",
      "SOC is less than 10%:  6.296863819293897\n",
      "SOC is less than 10%:  5.724846675301224\n",
      "SOC is less than 10%:  4.745265007019043\n",
      "Episode: 20, Reward: -1489.64, Best Reward: 10.38\n",
      "SOC is less than 10%:  4.907754898071289\n",
      "SOC is less than 10%:  9.745794773101807\n",
      "Episode: 21, Reward: -988.65, Best Reward: 10.38\n",
      "SOC is less than 10%:  9.359132766083231\n",
      "SOC is less than 10%:  6.007437133604569\n",
      "SOC is less than 10%:  9.122334448538599\n",
      "SOC is less than 10%:  9.445260625608558\n",
      "SOC is less than 10%:  5.105082988739014\n",
      "Episode: 22, Reward: -2486.93, Best Reward: 10.38\n",
      "Episode: 23, Reward: 13.36, Best Reward: 13.36\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  7.886004887038961\n",
      "SOC is less than 10%:  6.380451178718474\n",
      "Episode: 24, Reward: -1488.71, Best Reward: 13.36\n",
      "SOC is less than 10%:  3.139483874157605\n",
      "Episode: 25, Reward: -486.29, Best Reward: 13.36\n",
      "SOC is less than 10%:  4.273128806984321\n",
      "SOC is less than 10%:  5.41660770620323\n",
      "SOC is less than 10%:  6.15440743354568\n",
      "Episode: 26, Reward: -1490.04, Best Reward: 13.36\n",
      "SOC is less than 10%:  0.0\n",
      "Episode: 27, Reward: -493.76, Best Reward: 13.36\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  7.464543521404266\n",
      "SOC is less than 10%:  6.302252769470215\n",
      "SOC is less than 10%:  6.636993885040283\n",
      "Episode: 28, Reward: -2492.24, Best Reward: 13.36\n",
      "SOC is less than 10%:  6.759429715146666\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  6.567966461181641\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  6.655969876692847\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  7.640633702278137\n",
      "Episode: 29, Reward: -3490.04, Best Reward: 13.36\n",
      "Episode: 30, Reward: 9.04, Best Reward: 13.36\n",
      "SOC is less than 10%:  8.38197374343872\n",
      "SOC is less than 10%:  0.0\n",
      "Episode: 31, Reward: -994.1, Best Reward: 13.36\n",
      "SOC is less than 10%:  7.561931610107422\n",
      "Episode: 32, Reward: -492.3, Best Reward: 13.36\n",
      "SOC is less than 10%:  4.405115267191032\n",
      "SOC is less than 10%:  1.4961323377732185\n",
      "Episode: 33, Reward: -996.18, Best Reward: 13.36\n",
      "SOC is less than 10%:  9.73726192335959\n",
      "Episode: 34, Reward: -485.06, Best Reward: 13.36\n",
      "Episode: 35, Reward: 4.82, Best Reward: 13.36\n",
      "SOC is less than 10%:  3.777205313539305\n",
      "SOC is less than 10%:  8.894062042236328\n",
      "Episode: 36, Reward: -991.08, Best Reward: 13.36\n",
      "SOC is less than 10%:  0.0\n",
      "Episode: 37, Reward: -492.2, Best Reward: 13.36\n",
      "Episode: 38, Reward: 4.78, Best Reward: 13.36\n",
      "Episode: 39, Reward: 3.58, Best Reward: 13.36\n",
      "Episode: 40, Reward: 6.53, Best Reward: 13.36\n",
      "SOC is less than 10%:  0.0\n",
      "Episode: 41, Reward: -493.49, Best Reward: 13.36\n",
      "Episode: 42, Reward: 4.7, Best Reward: 13.36\n",
      "SOC is less than 10%:  7.773974340284564\n",
      "SOC is less than 10%:  3.9050435566138724\n",
      "SOC is less than 10%:  5.947722805499875\n",
      "SOC is less than 10%:  5.577110698282564\n",
      "Episode: 43, Reward: -1993.48, Best Reward: 13.36\n",
      "SOC is less than 10%:  7.533878564834595\n",
      "SOC is less than 10%:  5.749723446409465\n",
      "SOC is less than 10%:  5.857354842135003\n",
      "SOC is less than 10%:  7.532108169700766\n",
      "SOC is less than 10%:  9.45059385895729\n",
      "Episode: 44, Reward: -2496.27, Best Reward: 13.36\n",
      "Episode: 45, Reward: 1.92, Best Reward: 13.36\n",
      "Episode: 46, Reward: 7.42, Best Reward: 13.36\n",
      "SOC is less than 10%:  6.6002732423091\n",
      "SOC is less than 10%:  7.875037772883751\n",
      "SOC is less than 10%:  7.369694769382477\n",
      "SOC is less than 10%:  7.01758283842355\n",
      "SOC is less than 10%:  5.89351819860887\n",
      "Episode: 47, Reward: -2493.96, Best Reward: 13.36\n",
      "SOC is less than 10%:  8.809236828049464\n",
      "SOC is less than 10%:  9.29029494101097\n",
      "Episode: 48, Reward: -994.43, Best Reward: 13.36\n",
      "Episode: 49, Reward: 3.46, Best Reward: 13.36\n",
      "Episode: 50, Reward: 3.46, Best Reward: 13.36\n",
      "Episode: 51, Reward: 4.49, Best Reward: 13.36\n",
      "Episode: 52, Reward: 6.21, Best Reward: 13.36\n",
      "Episode: 53, Reward: 3.88, Best Reward: 13.36\n",
      "Episode: 54, Reward: 1.67, Best Reward: 13.36\n",
      "Episode: 55, Reward: 2.14, Best Reward: 13.36\n",
      "SOC is less than 10%:  8.58126747272047\n",
      "SOC is less than 10%:  9.73006693838818\n",
      "SOC is less than 10%:  8.382045760813188\n",
      "SOC is less than 10%:  9.49626529685509\n",
      "Episode: 56, Reward: -1995.02, Best Reward: 13.36\n",
      "SOC is less than 10%:  8.32346224784851\n",
      "SOC is less than 10%:  9.185870040208101\n",
      "Episode: 57, Reward: -998.21, Best Reward: 13.36\n",
      "Episode: 58, Reward: 3.35, Best Reward: 13.36\n",
      "Episode: 59, Reward: 2.93, Best Reward: 13.36\n",
      "Episode: 60, Reward: 1.99, Best Reward: 13.36\n",
      "Episode: 61, Reward: 2.42, Best Reward: 13.36\n",
      "Episode: 62, Reward: 0.78, Best Reward: 13.36\n",
      "Episode: 63, Reward: -0.89, Best Reward: 13.36\n",
      "Episode: 64, Reward: 6.06, Best Reward: 13.36\n",
      "Episode: 65, Reward: 3.18, Best Reward: 13.36\n",
      "Episode: 66, Reward: 2.37, Best Reward: 13.36\n",
      "SOC is less than 10%:  7.877389335899288\n",
      "SOC is less than 10%:  7.09930045333379\n",
      "SOC is less than 10%:  5.642698643623525\n",
      "SOC is less than 10%:  5.262915618612105\n",
      "SOC is less than 10%:  5.323802760696644\n",
      "SOC is less than 10%:  5.6224409834357445\n",
      "SOC is less than 10%:  5.759349389827484\n",
      "SOC is less than 10%:  5.5948074906008785\n",
      "SOC is less than 10%:  5.780735072108591\n",
      "SOC is less than 10%:  5.964654782215739\n",
      "SOC is less than 10%:  5.896990364311868\n",
      "SOC is less than 10%:  6.408492803573608\n",
      "Episode: 67, Reward: -5996.04, Best Reward: 13.36\n",
      "SOC is less than 10%:  6.406534682959318\n",
      "SOC is less than 10%:  6.355111779645085\n",
      "SOC is less than 10%:  6.795770407654345\n",
      "SOC is less than 10%:  6.736343815922737\n",
      "SOC is less than 10%:  6.918970841914415\n",
      "SOC is less than 10%:  7.125586387701333\n",
      "SOC is less than 10%:  9.389134821482003\n",
      "SOC is less than 10%:  9.515193617902696\n",
      "SOC is less than 10%:  9.157936348579824\n",
      "SOC is less than 10%:  8.705705066636739\n",
      "SOC is less than 10%:  8.135374956767578\n",
      "Episode: 68, Reward: -5498.22, Best Reward: 13.36\n",
      "SOC is less than 10%:  9.711560046323893\n",
      "SOC is less than 10%:  9.952997395792362\n",
      "SOC is less than 10%:  7.760294182755283\n",
      "SOC is less than 10%:  9.94220028591179\n",
      "Episode: 69, Reward: -1995.85, Best Reward: 13.36\n",
      "SOC is less than 10%:  8.3487498331815\n",
      "SOC is less than 10%:  9.289013657473227\n",
      "SOC is less than 10%:  7.871609151363373\n",
      "SOC is less than 10%:  9.379723658785224\n",
      "SOC is less than 10%:  5.132838059216738\n",
      "Episode: 70, Reward: -2499.25, Best Reward: 13.36\n",
      "Episode: 71, Reward: 5.58, Best Reward: 13.36\n",
      "Episode: 72, Reward: 4.12, Best Reward: 13.36\n",
      "Episode: 73, Reward: 4.39, Best Reward: 13.36\n",
      "Episode: 74, Reward: 5.18, Best Reward: 13.36\n",
      "SOC is less than 10%:  9.205392144822103\n",
      "SOC is less than 10%:  0.0\n",
      "Episode: 75, Reward: -996.08, Best Reward: 13.36\n",
      "SOC is less than 10%:  8.780639730268433\n",
      "SOC is less than 10%:  8.21629103245364\n",
      "Episode: 76, Reward: -997.69, Best Reward: 13.36\n",
      "SOC is less than 10%:  8.242991481490293\n",
      "Episode: 77, Reward: -494.21, Best Reward: 13.36\n",
      "Episode: 78, Reward: 5.28, Best Reward: 13.36\n",
      "Episode: 79, Reward: 4.5, Best Reward: 13.36\n",
      "Episode: 80, Reward: 8.69, Best Reward: 13.36\n",
      "Episode: 81, Reward: 3.47, Best Reward: 13.36\n",
      "Episode: 82, Reward: 2.29, Best Reward: 13.36\n",
      "Episode: 83, Reward: 1.49, Best Reward: 13.36\n",
      "Episode: 84, Reward: 4.48, Best Reward: 13.36\n",
      "Episode: 85, Reward: 3.27, Best Reward: 13.36\n",
      "Episode: 86, Reward: -1.21, Best Reward: 13.36\n",
      "Episode: 87, Reward: 4.36, Best Reward: 13.36\n",
      "Episode: 88, Reward: 3.39, Best Reward: 13.36\n",
      "Episode: 89, Reward: 2.67, Best Reward: 13.36\n",
      "Episode: 90, Reward: 5.18, Best Reward: 13.36\n",
      "Episode: 91, Reward: 3.54, Best Reward: 13.36\n",
      "Episode: 92, Reward: 2.2, Best Reward: 13.36\n",
      "Episode: 93, Reward: 4.33, Best Reward: 13.36\n",
      "Episode: 94, Reward: 5.03, Best Reward: 13.36\n",
      "Episode: 95, Reward: 1.35, Best Reward: 13.36\n",
      "Episode: 96, Reward: 3.71, Best Reward: 13.36\n",
      "Episode: 97, Reward: 3.95, Best Reward: 13.36\n",
      "Episode: 98, Reward: 3.99, Best Reward: 13.36\n",
      "Episode: 99, Reward: 3.83, Best Reward: 13.36\n",
      "Episode: 100, Reward: 3.11, Best Reward: 13.36\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "import os\n",
    "\n",
    "#initialize env\n",
    "env = ChargingEnv()\n",
    "STATE_DIM = env.observation_space.shape[0]\n",
    "ACTION_DIM = env.action_space.shape[0]\n",
    "\n",
    "agent = DDPGAgent(STATE_DIM, ACTION_DIM)\n",
    "\n",
    "# Hypperparameters\n",
    "NUM_EPISODE = 100\n",
    "NUM_STEP = 168\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY = 8400\n",
    "\n",
    "REWARD_BUFFER = np.empty(shape=NUM_EPISODE)\n",
    "\n",
    "best_reward = float('-inf')  # 初始化最佳奖励为负无穷\n",
    "\n",
    "for episode_i in range(NUM_EPISODE):\n",
    "    state, others = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step_i in range(NUM_STEP):\n",
    "        epsilon = np.interp(episode_i * NUM_STEP + step_i, [0, EPSILON_DECAY],\n",
    "                            [EPSILON_START, EPSILON_END])  # interpolation\n",
    "        random_sample = random.random()\n",
    "        if random_sample <= epsilon:\n",
    "            action = np.random.uniform(low=-20, high=20, size=ACTION_DIM)\n",
    "        else:\n",
    "            action = agent.get_action(state)\n",
    "        next_state, reward, done, truncation, info = env.step(action)\n",
    "        agent.replay_buffer.add_memo(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        agent.update()\n",
    "        if done:\n",
    "            break\n",
    "    REWARD_BUFFER[episode_i] = episode_reward\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward\n",
    "        # 保存模型\n",
    "        current_path = os.getcwd()\n",
    "        model_path = current_path + '/models'\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        torch.save(agent.actor.state_dict(), model_path + \"/best_actor_model.pth\")\n",
    "        torch.save(agent.critic.state_dict(), model_path + \"/best_critic_model.pth\")\n",
    "\n",
    "    print(f\"Episode: {episode_i + 1}, Reward: {round(episode_reward, 2)}, Best Reward: {round(best_reward, 2)}\")\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "model_path = current_path + '/models'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "torch.save(agent.actor.state_dict(), model_path + \"/ddpg_actor.pth\")\n",
    "torch.save(agent.critic.state_dict(), model_path + \"/ddpg_critic.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type:  cpu\n",
      "length of prices 44160\n",
      "Episode: 1, Reward: 3.61\n",
      "Episode: 2, Reward: 2.64\n",
      "Episode: 3, Reward: 2.51\n",
      "Episode: 4, Reward: 2.47\n",
      "Episode: 5, Reward: 2.18\n",
      "Episode: 6, Reward: 1.92\n",
      "Episode: 7, Reward: 2.02\n",
      "Episode: 8, Reward: 1.63\n",
      "Episode: 9, Reward: 2.03\n",
      "Episode: 10, Reward: 2.21\n",
      "Episode: 11, Reward: 2.24\n",
      "Episode: 12, Reward: 2.47\n",
      "Episode: 13, Reward: 2.95\n",
      "Episode: 14, Reward: 2.7\n",
      "Episode: 15, Reward: 2.53\n",
      "Episode: 16, Reward: 1.81\n",
      "Episode: 17, Reward: 2.13\n",
      "Episode: 18, Reward: 2.1\n",
      "Episode: 19, Reward: 2.18\n",
      "Episode: 20, Reward: 1.56\n",
      "Episode: 21, Reward: 2.12\n",
      "Episode: 22, Reward: 1.72\n",
      "Episode: 23, Reward: 2.17\n",
      "Episode: 24, Reward: 2.63\n",
      "Episode: 25, Reward: 2.59\n",
      "Episode: 26, Reward: 2.47\n",
      "Episode: 27, Reward: 2.78\n",
      "Episode: 28, Reward: 2.98\n",
      "Episode: 29, Reward: 2.93\n",
      "Episode: 30, Reward: 2.4\n",
      "Episode: 31, Reward: 0.03\n",
      "Episode: 32, Reward: 2.67\n",
      "Episode: 33, Reward: 2.72\n",
      "Episode: 34, Reward: 2.4\n",
      "Episode: 35, Reward: 2.62\n",
      "Episode: 36, Reward: 2.59\n",
      "Episode: 37, Reward: 1.4\n",
      "Episode: 38, Reward: 2.76\n",
      "Episode: 39, Reward: 2.63\n",
      "Episode: 40, Reward: 1.5\n",
      "Episode: 41, Reward: 2.46\n",
      "Episode: 42, Reward: 2.67\n",
      "Episode: 43, Reward: 2.18\n",
      "Episode: 44, Reward: 1.55\n",
      "Episode: 45, Reward: 1.29\n",
      "Episode: 46, Reward: 2.78\n",
      "Episode: 47, Reward: 3.27\n",
      "Episode: 48, Reward: 2.97\n",
      "Episode: 49, Reward: 2.71\n",
      "Episode: 50, Reward: 2.32\n",
      "Episode: 51, Reward: 2.21\n",
      "Episode: 52, Reward: 3.0\n",
      "Episode: 53, Reward: 2.57\n",
      "Episode: 54, Reward: 2.01\n",
      "Episode: 55, Reward: 2.33\n",
      "Episode: 56, Reward: 1.94\n",
      "Episode: 57, Reward: 1.64\n",
      "Episode: 58, Reward: 1.39\n",
      "Episode: 59, Reward: 1.82\n",
      "Episode: 60, Reward: 1.48\n",
      "Episode: 61, Reward: 2.0\n",
      "Episode: 62, Reward: 2.9\n",
      "Episode: 63, Reward: 2.58\n",
      "Episode: 64, Reward: 2.48\n",
      "Episode: 65, Reward: 1.48\n",
      "Episode: 66, Reward: 1.35\n",
      "Episode: 67, Reward: 3.18\n",
      "Episode: 68, Reward: 0.95\n",
      "Episode: 69, Reward: 1.55\n",
      "Episode: 70, Reward: 1.37\n",
      "Episode: 71, Reward: 3.26\n",
      "Episode: 72, Reward: 2.24\n",
      "Episode: 73, Reward: 1.39\n",
      "Episode: 74, Reward: 2.94\n",
      "Episode: 75, Reward: 0.73\n",
      "Episode: 76, Reward: 2.52\n",
      "Episode: 77, Reward: 2.9\n",
      "Episode: 78, Reward: 2.18\n",
      "Episode: 79, Reward: 2.93\n",
      "Episode: 80, Reward: 3.19\n",
      "Episode: 81, Reward: 3.0\n",
      "Episode: 82, Reward: 2.78\n",
      "Episode: 83, Reward: 2.33\n",
      "Episode: 84, Reward: 2.9\n",
      "Episode: 85, Reward: 1.68\n",
      "Episode: 86, Reward: 1.29\n",
      "Episode: 87, Reward: 3.24\n",
      "Episode: 88, Reward: 0.71\n",
      "Episode: 89, Reward: 2.9\n",
      "Episode: 90, Reward: 2.43\n",
      "Episode: 91, Reward: 2.26\n",
      "Episode: 92, Reward: 2.48\n",
      "Episode: 93, Reward: 2.12\n",
      "Episode: 94, Reward: 3.14\n",
      "Episode: 95, Reward: 2.9\n",
      "Episode: 96, Reward: 1.41\n",
      "Episode: 97, Reward: 3.01\n",
      "Episode: 98, Reward: 3.43\n",
      "Episode: 99, Reward: 2.03\n",
      "Episode: 100, Reward: 0.01\n",
      "Average Reward over 100 episodes: 2.27\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device type: \", device)\n",
    "\n",
    "# Initialize env\n",
    "env = ChargingEnv()\n",
    "STATE_DIM = env.observation_space.shape[0]\n",
    "ACTION_DIM = env.action_space.shape[0]\n",
    "\n",
    "# Load para\n",
    "current_path = os.getcwd()\n",
    "model = current_path + '/models/'\n",
    "actor_path = model + \"best_actor_model.pth\"\n",
    "# actor_path = model + \"ddpg_actor.pth\"\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x)) * 20\n",
    "        return x\n",
    "    \n",
    "actor = Actor(STATE_DIM, ACTION_DIM).to(device)\n",
    "actor.load_state_dict(torch.load(actor_path))\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EPISODE = 100\n",
    "NUM_STEP = 24\n",
    "\n",
    "# List to store episode rewards\n",
    "episode_rewards = []\n",
    "\n",
    "for episode_i in range(NUM_EPISODE):\n",
    "    state, others = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for tep_i in range(NUM_STEP):\n",
    "        action = actor(torch.FloatTensor(state).unsqueeze(0).to(device)).detach().cpu().numpy()[0]\n",
    "        next_state, reward, done, truncation, info = env.step(action)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    print(f\"Episode: {episode_i + 1}, Reward: {round(episode_reward, 2)}\")\n",
    "\n",
    "# Calculate average reward\n",
    "average_reward = np.mean(episode_rewards)\n",
    "print(f\"Average Reward over {NUM_EPISODE} episodes: {round(average_reward, 2)}\")\n",
    "\n",
    "#参数，一次循环中的参数，PPO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
