{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "class ChargingEnv():\n",
    "    \"\"\"\n",
    "    ### 动作空间\n",
    "\n",
    "    动作是形状为`(1,)`的`ndarray`, 表示该小时内汽车充放电的功率, 正数代表充电, 复数代表放电.\n",
    "\n",
    "    | 序号 | 动作     | 最小值 | 最大值 |\n",
    "    |-----|----------|--------|--------|\n",
    "    | 0   | power    | -20.0   | 20.0    |\n",
    "\n",
    "\n",
    "    ### 观察空间\n",
    "\n",
    "    观察是形状为`(2,)`的`ndarray`, 表示汽车的当前电量和当前电价。\n",
    "\n",
    "    | 序号 | 观察                | 最小值 | 最大值 |\n",
    "    |-----|--------------------|--------|--------|\n",
    "    | 0   | SOC                | 0      | 77     |\n",
    "    | 1   | e_price            | 30     | 120    |\n",
    "\n",
    "    ### 奖励\n",
    "\n",
    "    奖励函数定义为：\n",
    "\n",
    "    *r = -(power * e_price)*\n",
    "\n",
    "    ### 起始状态\n",
    "\n",
    "    起始状态是SOC为77(即满电状态), 电价取决于开始时的电价数据。\n",
    "    \"\"\"\n",
    "    def __init__(self, penalty_factor=0.1):\n",
    "        #环境参数\n",
    "        # 生成上班时间，范围在七点到九点\n",
    "        self.start_time = self.generate_random_time(7, 9)\n",
    "        # 生成下班时间，范围在四点到六点\n",
    "        self.end_time = self.generate_random_time(16, 18)\n",
    "        #计算出实际情况下的开始充电和停止充电时间(整点)\n",
    "        self.real_start_time, self.real_end_time = self.calculate_real_time(self.start_time, self.end_time)\n",
    "        self.battery_capacity = 77\n",
    "        self.SOC = 77\n",
    "        self.soc_min = 0.0\n",
    "        self.soc_max = 77.0\n",
    "        self.e_price_min = 0.0\n",
    "        self.e_price_max = 200.0\n",
    "        self.power_max = 20.0\n",
    "        self.current_step = 0\n",
    "        self.df_prices = pd.read_csv(\"GR-data-11-20.csv\", header=None, names=[\"DateTime\", \"ElectricityPrice\"])\n",
    "        self.penalty_factor = penalty_factor\n",
    "\n",
    "        print(\"length of prices\", len(self.df_prices))\n",
    "\n",
    "        # 观察空间和动作空间的定义\n",
    "        # 定义观察空间\n",
    "        self.observation_space = spaces.Box(low=np.array([self.soc_min, self.e_price_min]),\n",
    "                                            high=np.array([self.soc_max, self.e_price_max]),\n",
    "                                            dtype=np.float32)       \n",
    "\n",
    "        # 定义动作空间\n",
    "        self.action_space = spaces.Box(low=np.array([-self.power_max]),\n",
    "                                       high=np.array([self.power_max]),\n",
    "                                       dtype=np.float32)\n",
    "        \n",
    "    def generate_random_time(self, start_hour, end_hour):\n",
    "        hour = random.randint(start_hour, end_hour)\n",
    "        minute = random.randint(0, 59)\n",
    "        second = random.randint(0, 59)\n",
    "        return datetime.now().replace(hour=hour, minute=minute, second=second)\n",
    "    \n",
    "    def calculate_real_time(self, start_time, end_time):\n",
    "        real_start_time = (start_time + timedelta(hours = 1))\n",
    "        real_start_time = real_start_time.replace(minute=0, second=0)\n",
    "        real_end_time = (end_time - timedelta(hours = 0))\n",
    "        real_end_time = real_end_time.replace(minute=0, second=0)\n",
    "        return real_start_time, real_end_time\n",
    "        \n",
    "    def read_e_price(self,index):  \n",
    "        # Ensure the index is within the range of the dataframe\n",
    "        if 0 <= index < len(self.df_prices):\n",
    "            one_price = self.df_prices[\"ElectricityPrice\"].iloc[index] / 1000\n",
    "            return one_price\n",
    "        else:\n",
    "            # Handle the case where the index is out of range\n",
    "            print(\"Index out of range.\")\n",
    "            return None\n",
    "        \n",
    "    def step(self, power):\n",
    "        \"\"\"\n",
    "        在环境中执行一步动作，并返回新的观察、奖励等信息。\n",
    "\n",
    "        参数：\n",
    "        - `power`：该小时内汽车充电的功率。\n",
    "\n",
    "        返回：\n",
    "        - `observation`：新的观察。\n",
    "        - `reward`：当前步的奖励。\n",
    "        - `done`：标志是否完成（截断剧集）。\n",
    "        - `info`：其他信息（空字典）。\n",
    "        \"\"\"\n",
    "        SOC, e_price = self.state  # th := theta\n",
    "\n",
    "        start_time = self.start_time\n",
    "        end_time = self.end_time\n",
    "        battery_capacity = self.battery_capacity\n",
    "\n",
    "        # 对功率进行裁剪，确保在合理范围内\n",
    "        power = np.clip(power, -self.power_max, self.power_max)[0]\n",
    "        \n",
    "        # 计算新的SOC\n",
    "        newSOC = SOC + power\n",
    "        newSOC = np.clip(newSOC, self.soc_min, self.soc_max)\n",
    "        self.SOC = newSOC\n",
    "\n",
    "        # 计算成本，根据功率和电价\n",
    "        costs = (newSOC - SOC) * e_price\n",
    "        penalty = self.penalty_factor * min(0, power) * e_price\n",
    "        costs -= penalty\n",
    "\n",
    "        if (self.current_step + 1) % 24 == self.real_start_time.hour or (self.current_step + 1) % 24 == self.real_end_time.hour:\n",
    "            if (self.SOC < 10):\n",
    "                costs += 500\n",
    "                print(\"SOC is less than 10%: \", self.SOC)\n",
    "            \n",
    "        #取出新的电价\n",
    "        newe_price = self.read_e_price(self.current_step+1)\n",
    "        newe_price = np.clip(newe_price, self.e_price_min, self.e_price_max)\n",
    "        self.current_step += 1\n",
    "\n",
    "        self.state = np.array([newSOC, newe_price])\n",
    "\n",
    "        # 返回新的观察、奖励、是否完成、其他信息\n",
    "        return self._get_obs(), -costs, False, False, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境到初始状态。\n",
    "\n",
    "        返回：\n",
    "        - `observation`：初始观察。\n",
    "        - `info`：空字典。\n",
    "        \"\"\"\n",
    "        # 恢复起始状态\n",
    "        # self.current_step = 0\n",
    "        self.state = np.array([77, self.read_e_price(self.current_step)])\n",
    "        \n",
    "        # 返回初始观察和空字典\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        返回当前观察。\n",
    "\n",
    "        返回：\n",
    "        - `observation`：当前观察。\n",
    "        \"\"\"\n",
    "        SOC, e_price = self.state\n",
    "        return np.array([SOC,e_price], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TH\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Agent\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device type: \", device)\n",
    "\n",
    "# Hyperparameters\n",
    "LR_ACTOR = 1e-4\n",
    "LR_CRITIC = 1e-3\n",
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "TAU = 5e-3\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x)) * 20\n",
    "        return x\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add_memo(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "        self.replay_buffer = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = self.actor(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "        # Update critic\n",
    "        next_actions = self.actor_target(next_states)\n",
    "        target_Q = self.critic_target(next_states,\n",
    "                                      next_actions.detach())  # .detach() means the gradient won't be backpropagated to the actor\n",
    "        target_Q = rewards + (GAMMA * target_Q * (1 - dones))\n",
    "        current_Q = self.critic(states, actions)\n",
    "        critic_loss = nn.MSELoss()(current_Q, target_Q.detach())  # nn.MSELoss() means Mean Squared Error\n",
    "        self.critic_optimizer.zero_grad()  # .zero_grad() clears old gradients from the last step\n",
    "        critic_loss.backward()  # .backward() computes the derivative of the loss\n",
    "        self.critic_optimizer.step()  # .step() is to update the parameters\n",
    "\n",
    "        # Update actor \n",
    "        actor_loss = -self.critic(states, self.actor(states)).mean()  # .mean() is to calculate the mean of the tensor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target networks of critic and actor\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
    "            \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of prices 17664\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DDPGAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m STATE_DIM \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m ACTION_DIM \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDDPGAgent\u001b[49m(STATE_DIM, ACTION_DIM)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Hypperparameters\u001b[39;00m\n\u001b[0;32m     13\u001b[0m NUM_EPISODE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DDPGAgent' is not defined"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "import os\n",
    "\n",
    "#initialize env\n",
    "env = ChargingEnv()\n",
    "STATE_DIM = env.observation_space.shape[0]\n",
    "ACTION_DIM = env.action_space.shape[0]\n",
    "\n",
    "agent = DDPGAgent(STATE_DIM, ACTION_DIM)\n",
    "\n",
    "# Hypperparameters\n",
    "NUM_EPISODE = 100\n",
    "NUM_STEP = 168\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY = 8400\n",
    "\n",
    "REWARD_BUFFER = np.empty(shape=NUM_EPISODE)\n",
    "\n",
    "best_reward = float('-inf')  # 初始化最佳奖励为负无穷\n",
    "\n",
    "for episode_i in range(NUM_EPISODE):\n",
    "    state, others = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step_i in range(NUM_STEP):\n",
    "        epsilon = np.interp(episode_i * NUM_STEP + step_i, [0, EPSILON_DECAY],\n",
    "                            [EPSILON_START, EPSILON_END])  # interpolation\n",
    "        random_sample = random.random()\n",
    "        if random_sample <= epsilon:\n",
    "            action = np.random.uniform(low=-20, high=20, size=ACTION_DIM)\n",
    "        else:\n",
    "            action = agent.get_action(state)\n",
    "        next_state, reward, done, truncation, info = env.step(action)\n",
    "        agent.replay_buffer.add_memo(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        agent.update()\n",
    "        if done:\n",
    "            break\n",
    "    REWARD_BUFFER[episode_i] = episode_reward\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward\n",
    "        # 保存模型\n",
    "        current_path = os.getcwd()\n",
    "        model_path = current_path + '/models'\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        torch.save(agent.actor.state_dict(), model_path + \"/best_actor_model.pth\")\n",
    "        torch.save(agent.critic.state_dict(), model_path + \"/best_critic_model.pth\")\n",
    "\n",
    "    print(f\"Episode: {episode_i + 1}, Reward: {round(episode_reward, 2)}, Best Reward: {round(best_reward, 2)}\")\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "model_path = current_path + '/models'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "torch.save(agent.actor.state_dict(), model_path + \"/ddpg_actor.pth\")\n",
    "torch.save(agent.critic.state_dict(), model_path + \"/ddpg_critic.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type:  cpu\n",
      "length of prices 17664\n",
      "Episode: 1, Reward: 2.88\n",
      "Episode: 2, Reward: 1.38\n",
      "Episode: 3, Reward: 2.14\n",
      "Episode: 4, Reward: 1.51\n",
      "Episode: 5, Reward: 2.36\n",
      "Episode: 6, Reward: 2.13\n",
      "Episode: 7, Reward: 1.83\n",
      "Episode: 8, Reward: 1.9\n",
      "Episode: 9, Reward: 1.33\n",
      "Episode: 10, Reward: 1.79\n",
      "Episode: 11, Reward: 2.6\n",
      "Episode: 12, Reward: 1.86\n",
      "Episode: 13, Reward: 1.64\n",
      "Episode: 14, Reward: 1.91\n",
      "Episode: 15, Reward: 1.26\n",
      "Episode: 16, Reward: 1.81\n",
      "Episode: 17, Reward: 2.21\n",
      "Episode: 18, Reward: 1.99\n",
      "Episode: 19, Reward: 2.04\n",
      "Episode: 20, Reward: 2.47\n",
      "Episode: 21, Reward: 2.63\n",
      "Episode: 22, Reward: 3.14\n",
      "Episode: 23, Reward: 2.17\n",
      "Episode: 24, Reward: 2.91\n",
      "Episode: 25, Reward: 3.38\n",
      "Episode: 26, Reward: 2.84\n",
      "Episode: 27, Reward: 2.56\n",
      "Episode: 28, Reward: 1.71\n",
      "Episode: 29, Reward: 2.58\n",
      "Episode: 30, Reward: 1.59\n",
      "Episode: 31, Reward: 2.55\n",
      "Episode: 32, Reward: 2.43\n",
      "Episode: 33, Reward: 2.23\n",
      "Episode: 34, Reward: 2.94\n",
      "Episode: 35, Reward: 1.93\n",
      "Episode: 36, Reward: 2.17\n",
      "Episode: 37, Reward: 1.65\n",
      "Episode: 38, Reward: 2.27\n",
      "Episode: 39, Reward: 2.21\n",
      "Episode: 40, Reward: 2.25\n",
      "Episode: 41, Reward: 2.56\n",
      "Episode: 42, Reward: 1.72\n",
      "Episode: 43, Reward: 2.76\n",
      "Episode: 44, Reward: 2.38\n",
      "Episode: 45, Reward: 2.43\n",
      "Episode: 46, Reward: 2.82\n",
      "Episode: 47, Reward: 2.78\n",
      "Episode: 48, Reward: 3.14\n",
      "Episode: 49, Reward: 2.89\n",
      "Episode: 50, Reward: 3.1\n",
      "Episode: 51, Reward: 3.5\n",
      "Episode: 52, Reward: 3.33\n",
      "Episode: 53, Reward: 3.11\n",
      "Episode: 54, Reward: 2.15\n",
      "Episode: 55, Reward: 1.89\n",
      "Episode: 56, Reward: 1.9\n",
      "Episode: 57, Reward: 2.26\n",
      "Episode: 58, Reward: 2.61\n",
      "Episode: 59, Reward: 2.61\n",
      "Episode: 60, Reward: 2.72\n",
      "Episode: 61, Reward: 2.34\n",
      "Episode: 62, Reward: 1.47\n",
      "Episode: 63, Reward: 2.58\n",
      "Episode: 64, Reward: 2.79\n",
      "Episode: 65, Reward: 2.77\n",
      "Episode: 66, Reward: 1.23\n",
      "Episode: 67, Reward: 2.54\n",
      "Episode: 68, Reward: 1.43\n",
      "Episode: 69, Reward: 2.65\n",
      "Episode: 70, Reward: 2.31\n",
      "Episode: 71, Reward: 2.08\n",
      "Episode: 72, Reward: 2.71\n",
      "Episode: 73, Reward: 2.42\n",
      "Episode: 74, Reward: 2.72\n",
      "Episode: 75, Reward: 2.56\n",
      "Episode: 76, Reward: 2.85\n",
      "Episode: 77, Reward: 3.53\n",
      "Episode: 78, Reward: 3.39\n",
      "Episode: 79, Reward: 2.93\n",
      "Episode: 80, Reward: 2.39\n",
      "Episode: 81, Reward: 1.82\n",
      "Episode: 82, Reward: 1.92\n",
      "Episode: 83, Reward: 2.18\n",
      "Episode: 84, Reward: 2.27\n",
      "Episode: 85, Reward: 1.49\n",
      "Episode: 86, Reward: 1.48\n",
      "Episode: 87, Reward: 2.2\n",
      "Episode: 88, Reward: 1.37\n",
      "Episode: 89, Reward: 1.42\n",
      "Episode: 90, Reward: 2.06\n",
      "Episode: 91, Reward: 2.6\n",
      "Episode: 92, Reward: 1.5\n",
      "Episode: 93, Reward: 2.14\n",
      "Episode: 94, Reward: 0.46\n",
      "Episode: 95, Reward: 1.59\n",
      "Episode: 96, Reward: 2.27\n",
      "Episode: 97, Reward: 2.37\n",
      "Episode: 98, Reward: 2.62\n",
      "Episode: 99, Reward: 2.08\n",
      "Episode: 100, Reward: 2.72\n",
      "Average Reward over 100 episodes: 2.28\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device type: \", device)\n",
    "\n",
    "# Initialize env\n",
    "env = ChargingEnv()\n",
    "STATE_DIM = env.observation_space.shape[0]\n",
    "ACTION_DIM = env.action_space.shape[0]\n",
    "\n",
    "# Load para\n",
    "current_path = os.getcwd()\n",
    "model = current_path + '/models/'\n",
    "actor_path = model + \"best_actor_model.pth\"\n",
    "# actor_path = model + \"ddpg_actor.pth\"\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x)) * 20\n",
    "        return x\n",
    "    \n",
    "actor = Actor(STATE_DIM, ACTION_DIM).to(device)\n",
    "actor.load_state_dict(torch.load(actor_path))\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EPISODE = 100\n",
    "NUM_STEP = 168\n",
    "\n",
    "# List to store episode rewards\n",
    "episode_rewards = []\n",
    "\n",
    "for episode_i in range(NUM_EPISODE):\n",
    "    state, others = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for tep_i in range(NUM_STEP):\n",
    "        action = actor(torch.FloatTensor(state).unsqueeze(0).to(device)).detach().cpu().numpy()[0]\n",
    "        next_state, reward, done, truncation, info = env.step(action)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    print(f\"Episode: {episode_i + 1}, Reward: {round(episode_reward, 2)}\")\n",
    "\n",
    "# Calculate average reward\n",
    "average_reward = np.mean(episode_rewards)\n",
    "print(f\"Average Reward over {NUM_EPISODE} episodes: {round(average_reward, 2)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
