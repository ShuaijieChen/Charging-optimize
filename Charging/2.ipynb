{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "class ChargingEnv():\n",
    "    \"\"\"\n",
    "    ### 动作空间\n",
    "\n",
    "    动作是形状为`(1,)`的`ndarray`, 表示该小时内汽车充放电的功率, 正数代表充电, 复数代表放电.\n",
    "\n",
    "    | 序号 | 动作     | 值 |\n",
    "    |-----|----------|--------|\n",
    "    | 0   | discharge| 20.0   |\n",
    "    | 1   | stay     | 20.0   |\n",
    "    | 2   | charge   | 20     |\n",
    "\n",
    "\n",
    "    ### 观察空间\n",
    "\n",
    "    观察是形状为`(2,)`的`ndarray`, 表示汽车的当前电量和当前电价。\n",
    "\n",
    "    | 序号 | 观察                | 最小值 | 最大值 |\n",
    "    |-----|--------------------|--------|--------|\n",
    "    | 0   | SOC                | 0      | 77     |\n",
    "    | 1   | e_price            | 30     | 120    |\n",
    "\n",
    "    ### 奖励\n",
    "\n",
    "    奖励函数定义为：\n",
    "\n",
    "    *r = -(power * e_price)*\n",
    "\n",
    "    ### 起始状态\n",
    "\n",
    "    起始状态是SOC为77(即满电状态), 电价取决于开始时的电价数据。\n",
    "    \"\"\"\n",
    "    def __init__(self, penalty_factor=0.1):\n",
    "        #环境参数\n",
    "        # 生成上班时间，范围在七点到九点\n",
    "        self.start_time = self.generate_random_time(7, 9)\n",
    "        # 生成下班时间，范围在四点到六点\n",
    "        self.end_time = self.generate_random_time(16, 18)\n",
    "        #计算出实际情况下的开始充电和停止充电时间(整点)\n",
    "        self.real_start_time, self.real_end_time = self.calculate_real_time(self.start_time, self.end_time)\n",
    "        self.battery_capacity = 77\n",
    "        self.SOC = 77\n",
    "        self.soc_min = 0.0\n",
    "        self.soc_max = 77.0\n",
    "        self.e_price_min = 0.0\n",
    "        self.e_price_max = 200.0\n",
    "        self.power_max = 20.0\n",
    "        self.current_step = 0\n",
    "        self.df_prices = pd.read_csv(\"GR-data-11-20.csv\", header=None, names=[\"DateTime\", \"ElectricityPrice\"])\n",
    "        self.penalty_factor = penalty_factor\n",
    "\n",
    "        print(\"length of prices\", len(self.df_prices))\n",
    "\n",
    "        # 观察空间和动作空间的定义\n",
    "        # 定义观察空间\n",
    "        self.observation_space = spaces.Box(low=np.array([self.soc_min, self.e_price_min]),\n",
    "                                            high=np.array([self.soc_max, self.e_price_max]),\n",
    "                                            dtype=np.float32)       \n",
    "\n",
    "        # 定义动作空间\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def action_sample():\n",
    "        return random.randint(0, 2)\n",
    "        \n",
    "    def generate_random_time(self, start_hour, end_hour):\n",
    "        hour = random.randint(start_hour, end_hour)\n",
    "        minute = random.randint(0, 59)\n",
    "        second = random.randint(0, 59)\n",
    "        return datetime.now().replace(hour=hour, minute=minute, second=second)\n",
    "    \n",
    "    def calculate_real_time(self, start_time, end_time):\n",
    "        real_start_time = (start_time + timedelta(hours = 1))\n",
    "        real_start_time = real_start_time.replace(minute=0, second=0)\n",
    "        real_end_time = (end_time - timedelta(hours = 0))\n",
    "        real_end_time = real_end_time.replace(minute=0, second=0)\n",
    "        return real_start_time, real_end_time\n",
    "        \n",
    "    def read_e_price(self,index):  \n",
    "        # Ensure the index is within the range of the dataframe\n",
    "        if 0 <= index < len(self.df_prices):\n",
    "            one_price = self.df_prices[\"ElectricityPrice\"].iloc[index] / 1000\n",
    "            return one_price\n",
    "        else:\n",
    "            # Handle the case where the index is out of range\n",
    "            print(\"Index out of range.\", index)\n",
    "            return None\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        在环境中执行一步动作，并返回新的观察、奖励等信息。\n",
    "\n",
    "        参数：\n",
    "        - `power`：该小时内汽车充电的功率。\n",
    "\n",
    "        返回：\n",
    "        - `observation`：新的观察。\n",
    "        - `reward`：当前步的奖励。\n",
    "        - `done`：标志是否完成（截断剧集）。\n",
    "        - `info`：其他信息（空字典）。\n",
    "        \"\"\"\n",
    "        SOC, e_price = self.state  \n",
    "\n",
    "        # 计算新的SOC\n",
    "        newSOC = SOC + 20 * (action - 1)\n",
    "        newSOC = np.clip(newSOC, self.soc_min, self.soc_max)\n",
    "        self.SOC = newSOC\n",
    "\n",
    "        # 计算成本，根据功率和电价\n",
    "        costs = (newSOC - SOC) * e_price\n",
    "        penalty = self.penalty_factor * min(0, 20 * (action-1)) * e_price\n",
    "        costs -= penalty\n",
    "\n",
    "        if (self.current_step + 1) % 24 == self.real_start_time.hour or (self.current_step + 1) % 24 == self.real_end_time.hour:\n",
    "            if (self.SOC < 10):\n",
    "                costs += 500\n",
    "                print(\"SOC is less than 10%: \", self.SOC)\n",
    "            \n",
    "        #取出新的电价\n",
    "        newe_price = self.read_e_price(self.current_step+1)\n",
    "        newe_price = np.clip(newe_price, self.e_price_min, self.e_price_max)\n",
    "        self.current_step += 1\n",
    "\n",
    "        self.state = np.array([newSOC, newe_price])\n",
    "\n",
    "        # 返回新的观察、奖励、是否完成、其他信息\n",
    "        return self._get_obs(), -costs, False, False, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境到初始状态。\n",
    "\n",
    "        返回：\n",
    "        - `observation`：初始观察。\n",
    "        - `info`：空字典。\n",
    "        \"\"\"\n",
    "        # 恢复起始状态\n",
    "        # self.current_step = 0\n",
    "        self.state = np.array([77, self.read_e_price(self.current_step)])\n",
    "        \n",
    "        # 返回初始观察和空字典\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        返回当前观察。\n",
    "\n",
    "        返回：\n",
    "        - `observation`：当前观察。\n",
    "        \"\"\"\n",
    "        SOC, e_price = self.state\n",
    "        return np.array([SOC,e_price], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of prices 44160\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "SOC is less than 10%:  0.0\n",
      "平均分数 :  1.34\n"
     ]
    }
   ],
   "source": [
    "# 初始化\n",
    "num_soc_bins = 77\n",
    "num_e_price_bins = 200\n",
    "max_iteration = 100  # 最大迭代次数\n",
    "initial_learning_rate = 1.0  # 初始学习率\n",
    "min_learning_rate = 0.005  # 最小学习率\n",
    "max_step = 168  # 最大步数\n",
    "\n",
    "# Q-learning参数\n",
    "epsilon = 0.05  # epsilon-greedy策略中的探索率\n",
    "gamma = 1.0  # 折扣因子\n",
    "\n",
    "def observation_to_state(environment, observation):\n",
    "    # 获取观察空间的最低值\n",
    "    environment_low = np.array([environment.soc_min, environment.e_price_min], dtype=np.float32)\n",
    "    # 获取观察空间的最高值\n",
    "    environment_high = np.array([environment.soc_max, environment.e_price_max], dtype=np.float32)\n",
    "    # 计算在每个维度上的离散步长\n",
    "    environment_dx = (environment_high - environment_low) / number_states\n",
    "\n",
    "    # observation[0]:SOC ;  observation[1]: e_price\n",
    "    soc = int((observation[0] - environment_low[0])/environment_dx[0])\n",
    "    e_price = int((observation[1] - environment_low[1])/environment_dx[1])\n",
    "    # soc:SOC, e_price:e_price\n",
    "    return soc, e_price\n",
    "\n",
    "\n",
    "# 模拟一个回合的函数\n",
    "def episode_simulation(environment, policy=None):\n",
    "    observation, others = environment.reset()\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    for _ in range(max_step):\n",
    "        if policy is None:\n",
    "            action = environment.action_sample()\n",
    "        else:\n",
    "            soc, e_price = observation_to_state(environment, observation)\n",
    "            action = policy[soc][e_price]\n",
    "        observation, reward, done, trun, _ = environment.step(action)\n",
    "        total_reward += gamma ** step_count * reward\n",
    "        step_count += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    environment = ChargingEnv()\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # 创建Q表，并初始化为零\n",
    "    # 3个动作: 0: 左推, 1: 不推, 2: 右推\n",
    "    q_table = np.zeros((num_soc_bins, num_e_price_bins, 3))\n",
    "\n",
    "    # 训练max_iteration次\n",
    "    for i in range(max_iteration):\n",
    "        observation, others = environment.reset()\n",
    "        total_reward = 0\n",
    "        eta = max(min_learning_rate, initial_learning_rate * (0.85 ** (i // 100)))\n",
    "        for j in range(max_step):\n",
    "            soc, e_price = observation_to_state(environment, observation)\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.choice(environment.action_space.n)\n",
    "            else:\n",
    "                logits = q_table[soc][e_price]\n",
    "                logits_exp = np.exp(logits)\n",
    "                probabilities = logits_exp / np.sum(logits_exp)\n",
    "                action = np.random.choice(environment.action_space.n, p=probabilities)\n",
    "                observation, reward, done, trun, _ = environment.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "            soc_, e_price_ = observation_to_state(environment, observation)\n",
    "            q_table[soc][e_price][action] = q_table[soc][e_price][action] + eta * (\n",
    "                    reward + gamma * np.max(q_table[soc_][e_price_]) - q_table[soc][e_price][action])\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # 获取最优策略\n",
    "    solution_policy = np.argmax(q_table, axis=2)\n",
    "    solution_policy_scores = [episode_simulation(environment, solution_policy) for _ in range(100)]\n",
    "    print(\"平均分数 : \", round(np.mean(solution_policy_scores), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
